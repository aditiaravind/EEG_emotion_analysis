{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB_HBao5ggBj"
   },
   "source": [
    "# Machine Learning for EEG Dataset : DEAP\n",
    "\n",
    "http://www.eecs.qmul.ac.uk/mmv/datasets/deap/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnS00QIQggBp"
   },
   "source": [
    "## Training Models and Obtaining Accuracies for Subject Independent Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7rNAucHggBr"
   },
   "source": [
    "### Dataset (per Subject)\n",
    "\n",
    "nFeatures = 18 for each frequency band <br>\n",
    "nFeatures for each trial = 32 (electrode channels) x 5(Freq bands) x 18 features\n",
    "                         <br> =  2880 features per trial per patient\n",
    "\n",
    "This is reshaped into - \n",
    "- trials(40) x electrode channels(32) , Frequency Bands(5) x nFeatures (18)\n",
    "- Shape is (1280, 90) \n",
    "- 1280 samples and 90 features per sample\n",
    "\n",
    "<b> Now there are 32 such files, that get combined to give a final dataset shape of:\n",
    "- (40960, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--JD1dqAggBr"
   },
   "source": [
    "## Model\n",
    "\n",
    "### Input Data --> Reshape --> Impute NaN Values --> Feature Elimination --> Classification --> Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiAz6cGIggBs"
   },
   "source": [
    "For the last 2 steps, there are multiple options as follows,\n",
    "\n",
    "#### Feature Elimination:\n",
    "- LDA\n",
    "- RFE\n",
    "- PCA\n",
    "- FA\n",
    "\n",
    "#### Classification:\n",
    "- SVM\n",
    "  - Linear kernel\n",
    "  - Rbf kernel\n",
    "  - Polynomial kernel\n",
    "  - Signmoid kernel\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- K Nearest Neighbours (KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCSHypGz5LSF"
   },
   "source": [
    "# Subject Independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2wncf-S8fha"
   },
   "source": [
    "I recommend trying these out only on Google Colab - truly too heavy and too large for a normal CPU to handle, and having GPUs won't help either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo7HvcbksuLn"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTf6hN6z5hR1"
   },
   "outputs": [],
   "source": [
    "#If not in colab - run this in cmd and restart jupyter notebook \n",
    "!pip install tornado==5 distributed==2.4.0 dask-ml[complete]\n",
    "!python -m pip install dask[dataframe] --upgrade\n",
    "#Restart runtime once after running this cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyaX5DSU5Z7g",
    "outputId": "f04650e1-23fd-429b-a24c-8464b797963d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
      "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "#If not in colab - run this in cmd and restart jupyter notebook \n",
    "!pip install -U scikit-learn\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1WnkjUcz5Z7q",
    "outputId": "da588bc0-bab7-4588-f149-f33e4c5d4f72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure the sklearn version is the latest\n",
    "import sklearn\n",
    "sklearn.__version__\n",
    "#Should be 0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOsMlyFvtwPJ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oQ0F3df5Z7q"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from shutil import rmtree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H76jcrSb5hR1"
   },
   "outputs": [],
   "source": [
    "#If you don't want to use dask-ml, replace all dask_ml with sklearn. All commands are the same\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import joblib\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dask_ml.wrappers import Incremental, ParallelPostFit\n",
    "\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.styles.fills import PatternFill\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSWlBGea5Z7r",
    "outputId": "de8f9d2c-65c8-4d5a-bc57-a4ab5ef4fe5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Only run if importing features data and data generator from google drive\n",
    "#If you're using Google colab, like I am, you will have to import everything from google drive.\n",
    "#Uploading locally will take too much time, and you'll have to upload every time you open the notebook or restart runtime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX67D6CS5Z7r"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/content/drive/MyDrive/Upwork/') #Path to folder containing DataGenerator File\n",
    "from DataGenerator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuYrh-v883Hi"
   },
   "source": [
    "## Functions for Saving to Excel file and formatting (optional)\n",
    "Not important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4_PbF7o5yO9"
   },
   "outputs": [],
   "source": [
    "#Following are 2 variables used in the formatting of the excel file\n",
    "border = Border(left=Side(style='thin'), right=Side(style='thin'), \n",
    "                     top=Side(style='thin'), bottom=Side(style='thin'))\n",
    "col = dict(zip(list(range(1, 18)), list('ABCDEFGHIJ')))\n",
    "\n",
    "#Function that applies the below formatting function to all worksheets in the workbook \n",
    "def apply_formatting(wb, metric):\n",
    "    if wb.worksheets[0].title==\"Sheet\":\n",
    "        wb.remove(wb.worksheets[0])\n",
    "    for ws in wb.worksheets:\n",
    "        ws['A1'] = metric\n",
    "        A = pd.DataFrame(ws.values).iloc[1:, 1:].to_numpy()\n",
    "        bold_idx = np.where(A == np.nanmax(A))\n",
    "        bold_cells = [col[i+2] + str(j+2) for i,j in zip(bold_idx[1], bold_idx[0])]\n",
    "        bold_colour(ws, bold_cells)\n",
    "        apply_border(ws)\n",
    "\n",
    "\n",
    "#Functions for formatting the excel sheet - to apply borders\n",
    "def apply_border(sheet):\n",
    "    cells = [i + '6' for i in 'BCDEFGHIJ']\n",
    "    cells = cells + ['J' + str(i) for i in range(2,7)] + [i + '1' for i in 'ABCDEFGHIJ'] + ['A' + str(i) for i in range(2,7)]\n",
    "    for cell in cells:\n",
    "        sheet[cell].border = border\n",
    "\n",
    "#Functions for formatting the excel sheet - to highlight the cells of max accuracy        \n",
    "def bold_colour(sheet, cells):\n",
    "    for cell in cells + [i + '1' for i in 'ABCDEFGHIJ'] + ['A' + str(i) for i in range(2,7)]:\n",
    "        sheet[cell].font = Font(bold=True)\n",
    "    CC = [c for c in cells if c[0]!='J' and c[1]!='6'] \n",
    "    my_fill = PatternFill(patternType='solid', fgColor='b6d7a8')\n",
    "    for cell in CC + ['A1']:\n",
    "        sheet[cell].fill = my_fill\n",
    "# Function that converts a pandas dataframe to an excel worksheet\n",
    "def df2sheet(df, sheet):\n",
    "    for r in dataframe_to_rows(df, index=True, header=True):\n",
    "        sheet.append(r)\n",
    "    if not sheet['A2'].value:\n",
    "        sheet.delete_rows(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPisX8-ZqNoG"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDewjzMb5I0W"
   },
   "outputs": [],
   "source": [
    "#Replace datapath to the folder containing the final features and add \"/feats\" to the end of the path\n",
    "#Replace metapath with the path to \"participant_questionnaire.csv\"\n",
    "D = DataGenerator(datapath=\"/content/drive/MyDrive/Upwork/Final_features/feats\", metapath=\"/content/drive/MyDrive/Upwork/participant_questionnaire.csv\")\n",
    "\n",
    "data, labels = D.gen_data(mode='s_indep') #Data mode: s_indep : gives all the data from all patients in concatenated form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXXRfHEw5JFO",
    "outputId": "ace931a0-af05-485f-9825-f1507c8a5689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40960, 90), (40960, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1L5kcp1cqRcy"
   },
   "source": [
    "## Create necessary variables\n",
    "- Open the Multiprocessing client\n",
    "- Open the excel sheets to save data to\n",
    "- Create variables containing all the required Models\n",
    "- Create the pipeline for final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HkzWWs46E3F"
   },
   "outputs": [],
   "source": [
    "#For multi-processing for dask-ml , just to speed up the model training and scoring\n",
    "#Reduce the memory limit to 2GB if you are not using google colab\n",
    "#But for this Subject Independent Model, a limit of over 40GB is required, which isn't available in most normal CPUs\n",
    "client = Client(processes=False, memory_limit='25GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiOztHNo9IXz"
   },
   "outputs": [],
   "source": [
    "#Open a new Excel file to save all the data\n",
    "acc = Workbook()\n",
    "f1 = Workbook()\n",
    "\n",
    "# If you want to append sheets to an existing excel file use the following code\n",
    "# acc = load_workbook('path to acc excel file')\n",
    "# f1 = load_workbook('path to f1 excel file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJiaxMdg6E6x"
   },
   "outputs": [],
   "source": [
    "#Dictionary containing the Feature Reduction Functions for 75% and 25% features \n",
    "FeatureScalers = {\n",
    "        '0.75':{'LDA': LDA(), \n",
    "                'RFE': RFE(svm.SVC(kernel='linear', cache_size=7000),n_features_to_select=0.75), \n",
    "                'PCA': PCA(n_components=int(0.75*90)), \n",
    "                'FA': FactorAnalysis(n_components=int(0.75*90))},\n",
    "        \n",
    "        '0.25':{'LDA': LDA(), \n",
    "                'RFE': RFE(svm.SVC(kernel='linear', cache_size=7000),n_features_to_select=0.25), \n",
    "                'PCA': PCA(n_components=int(0.25*90)), \n",
    "                'FA': FactorAnalysis(n_components=int(0.25*90))}\n",
    "                } \n",
    "    \n",
    "#Dictionary containing the Final Classifiers to be used \n",
    "Classifiers = {\n",
    "        'SVM_linear': svm.LinearSVC(), \n",
    "        'SVM_rbf': svm.SVC(kernel='rbf', cache_size=7000),\n",
    "        'SVM_poly': svm.SVC(kernel='poly', cache_size=7000), \n",
    "        'SVM_sigmoid': svm.SVC(kernel='sigmoid', cache_size=7000),\n",
    "        'DecTree': DecisionTreeClassifier(), \n",
    "        'LogReg': LogisticRegression(solver='liblinear'), \n",
    "        'GNB': GaussianNB(), \n",
    "        'KNN': KNeighborsClassifier()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWMBJ7Bv6FR6"
   },
   "outputs": [],
   "source": [
    "#Cache storage to speed up calculations - requires a lot of space\n",
    "cache = \"/content/Cache\"\n",
    "\n",
    "#Form one path of pipeline to use as input estimator\n",
    "pipe = Pipeline(steps=[\n",
    "                  ('Impute', SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=0)),\n",
    "                  ('Scale', StandardScaler()),\n",
    "                  ('Feature_Elim', PCA()),\n",
    "                  ('Classifier', svm.LinearSVC())], \n",
    "                memory = cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDysAJUTpU0_"
   },
   "source": [
    "### Note: Pipelines\n",
    "- Pipelines are essentially a series of models that are executed one after another.\n",
    "- If you remember our model, there were multiple stages:\n",
    "  - Data Extraction\n",
    "  - Imputing NaN Values\n",
    "  - Scaling\n",
    "  - Feature Reduction\n",
    "  - Classification\n",
    "- A pipeline can add individual models for each of these steps \n",
    "  - Creates an overall model that ensures all of the steps are executed sequentially\n",
    "  - There's no hassle of executing each step individually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vhB9tZKqiVB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zOwqzldqkZg"
   },
   "source": [
    "## Model Execution and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYfDqowg8kTW"
   },
   "source": [
    "### 75% Features, Label 0 - Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3sR0Ft_96YX"
   },
   "outputs": [],
   "source": [
    "'''Change the score variable accordingly: \n",
    "  0 - Valence\n",
    "  1 - Arousal\n",
    "  2 - Dominance\n",
    "'''\n",
    "score = 0 #Choosing only the Valence scores\n",
    "X = data\n",
    "y = labels[:, score] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsuQpYhD60nu"
   },
   "outputs": [],
   "source": [
    "# Create a grid of options corresponding the the different classifiers and feature reduction methods\n",
    "# !!!! Change Feature value to 0.25 for 25% Features\n",
    "param_grid = {'Classifier':list(Classifiers.values()), \n",
    "              'Feature_Elim':list(FeatureScalers['0.75'].values()) #!!!! Change Feature value HERE to 0.25 for 25% Features\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7XthMrbYvDN",
    "outputId": "4dbf0824-ceb2-4c8b-80d1-03f98d5dfb7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Classifier': [LinearSVC(),\n",
       "  SVC(cache_size=7000),\n",
       "  SVC(cache_size=7000, kernel='poly'),\n",
       "  SVC(cache_size=7000, kernel='sigmoid'),\n",
       "  DecisionTreeClassifier(),\n",
       "  LogisticRegression(solver='liblinear'),\n",
       "  GaussianNB(),\n",
       "  KNeighborsClassifier()],\n",
       " 'Feature_Elim': [LinearDiscriminantAnalysis(),\n",
       "  PCA(n_components=67),\n",
       "  FactorAnalysis(n_components=67)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This grid is used in the cross-validation scheme to get all the scores of all the models at once\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcdP9KNC60qF"
   },
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'f1']\n",
    "\n",
    "splits = 10 #Change this to 5 if taking too much time/space\n",
    "\n",
    "grid_search = GridSearchCV(copy.copy(pipe), #Input estimator - our pipeline - take only a copy of it to ensure no overlapping errors\n",
    "                           param_grid=param_grid, #Applies cross-val for all the combinations of the param_grid of the pipeline\n",
    "                           cv=splits, #Uses StratifiedKFold cross validation\n",
    "                           return_train_score=False, \n",
    "                           refit=False,\n",
    "                           verbose=2, #Reduce this if you don't want to see intermediate outputs; Increase if you want to see more outputs\n",
    "                           scoring=scoring, #Multi-metric - Accuracy and F1 scores are calculated and returned together\n",
    "                           n_jobs=-1) #Use all available cpu cores in the system for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk8uTcyU60td"
   },
   "outputs": [],
   "source": [
    "\n",
    "with joblib.parallel_backend('dask'): #This line uses the dask-ml computation to speed up calculations\n",
    "      grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVelnVIA8xFH"
   },
   "outputs": [],
   "source": [
    "#Get the results\n",
    "CV = grid_search.cv_results_\n",
    "acc_results = np.round(CV['mean_test_accuracy'], 4).reshape(3, 8, order='F')\n",
    "acc_results = np.insert(acc_results, 1, 0, axis=0)\n",
    "f1_results = np.round(CV['mean_test_f1'], 4).reshape(3, 8, order='F')\n",
    "f1_results = np.insert(f1_results, 1, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQnvMGtyYPbh"
   },
   "outputs": [],
   "source": [
    "# Put the results into a dataframe and view\n",
    "A_df = pd.DataFrame(acc_results, index = FeatureScalers['0.75'].keys(), columns = Classifiers.keys())\n",
    "F_df = pd.DataFrame(f1_results, index = FeatureScalers['0.75'].keys(), columns = Classifiers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv1NooREnF6B"
   },
   "outputs": [],
   "source": [
    "A_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2G-iZaoXdi8"
   },
   "outputs": [],
   "source": [
    "F_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30stUaHj-Q8Q"
   },
   "outputs": [],
   "source": [
    "#Calculate the maximum across both axes\n",
    "A_df['Max'] = A_df.max(axis=1)\n",
    "A_df.loc['Max'] = A_df.max()\n",
    "\n",
    "F_df['Max'] = F_df.max(axis=1)\n",
    "F_df.loc['Max'] = F_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_ig0Ixg-okS"
   },
   "outputs": [],
   "source": [
    "A_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhwFcWad-qQf"
   },
   "outputs": [],
   "source": [
    "F_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5i0j9Ft9nHZ"
   },
   "outputs": [],
   "source": [
    "#Create a sheet in the open Workbooks with the correct name\n",
    "#!!!! Change the name for different sheets\n",
    "wA = acc.create_sheet(\"75% Val\")\n",
    "wF = f1.create_sheet(\"75% Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7RxNudf-Q_s"
   },
   "outputs": [],
   "source": [
    "# Apply formatting to the sheets\n",
    "df2sheet(A_df, wA)\n",
    "df2sheet(F_df, wF)\n",
    "\n",
    "apply_formatting(acc, 'Acc')\n",
    "apply_formatting(f1, 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThhiVnpB-yVU"
   },
   "outputs": [],
   "source": [
    "#Save Workbook to excel file \n",
    "# !!! Remember to change names as and when you save\n",
    "path = \"/content/drive/MyDrive/Upwork/Results/Subject Independent/\"\n",
    "\n",
    "acc.save(path + 'S_Indep_Acc.xlsx')\n",
    "f1.save(path + 'S_Indep_F1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbnjJroXm3BY"
   },
   "source": [
    "**Similarly, change the index to 1 or 2 to get Arousal and Dominance data\n",
    "and features to 25%**\n",
    " <br>\n",
    " Change the values and names accordingly to get the 6 different sheet combinations and save to excel file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ElXr2FusbrU"
   },
   "source": [
    "Subject Independent takes a really long time. <br> \n",
    "**8 hours or more** per sheet, so I advise against executing this on Jupyter Notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Subject Independent.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
