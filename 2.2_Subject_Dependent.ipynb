{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB_HBao5ggBj"
   },
   "source": [
    "# Machine Learning for EEG Dataset : DEAP\n",
    "\n",
    "http://www.eecs.qmul.ac.uk/mmv/datasets/deap/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnS00QIQggBp"
   },
   "source": [
    "## Training Models and Obtaining Accuracies for Subject Dependent Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7rNAucHggBr"
   },
   "source": [
    "### Dataset (per Subject)\n",
    "\n",
    "nFeatures = 18 for each frequency band <br>\n",
    "nFeatures for each trial = 32 (electrode channels) x 5(Freq bands) x 18 features\n",
    "                         <br> <b>=  2880 features per trial per patient\n",
    "\n",
    "This is reshaped into - \n",
    "- trials(40) x electrode channels(32) , Frequency Bands(5) x nFeatures (18)\n",
    "- <b> Final shape is (1280, 90) \n",
    "- 1280 samples and 90 features per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--JD1dqAggBr"
   },
   "source": [
    "## Model\n",
    "\n",
    "### Input Data --> Reshape --> Impute NaN Values --> Feature Elimination --> Classification --> Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiAz6cGIggBs"
   },
   "source": [
    "For the last 3 steps, there are multiple options as follows,\n",
    "\n",
    "#### Impute NaN values:\n",
    "- Use constant 0 value substitution\n",
    "- Use mean of the column to substitute\n",
    "- Use median of the column to substitute\n",
    "\n",
    "#### Feature Elimination:\n",
    "- PCA\n",
    "- RFE\n",
    "\n",
    "#### Classification:\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- K Nearest Neighbours (KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKDaB980zdRc"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9kHZSCbg1rR",
    "outputId": "1a9f1b54-d1f0-49a5-af01-cdde5dbb5d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "#If not in colab - run this in cmd and restart jupyter notebook \n",
    "!pip install -U scikit-learn\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hLqEKxiYggBt",
    "outputId": "0582a6d6-8274-4181-f475-69300339f5c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure the sklearn version is the latest\n",
    "import sklearn\n",
    "sklearn.__version__\n",
    "#Should be 0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Otmr0ckzyoX"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odIVzINwggBs"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from shutil import rmtree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47hOeM73ggBt"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.neighbors import KNeighborsTransformer, KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.styles.fills import PatternFill\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CtMVGE1hD2f",
    "outputId": "4fa60a69-5eef-46e5-da29-2299b5b48aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Only run if importing features data and data generator from google drive\n",
    "#If you're using Google colab, like I am, you will have to import everything from google drive.\n",
    "#Uploading locally will take too much time, and you'll have to upload every time you open the notebook or restart runtime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lFWPmEWggBu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/content/drive/MyDrive/Upwork/') #Path to folder containing DataGenerator File\n",
    "from DataGenerator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL6VhvHez6aM"
   },
   "source": [
    "## Custom Model\n",
    "It is a class that calculates all the scores of accuracy and f1 across all model pipelines and saves it to an excel file at the designated path with the required name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVZCYnPoggBu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In a class, the word \"self\" refers to an instance of this class.\n",
    "- Functions and variables that belong to the class are saved as \"self.someVariable\" and \"someFunction(self, ...)\"\n",
    "- The functions inside the class are called by self.someFunction(...)\n",
    "\n",
    "- Outside the class, when running in normal code, the class functions are called as follows:\n",
    "  - Obj = ClassName(...) [Obj is now an instance or Object of the class]\n",
    "  - Obj.someFunction(...) [Obj has all the functions - execute the functions by calling them as part of the object]\n",
    "  - Obj.someVariable [Any variable of the class can be accessed like this]\n",
    "\n",
    "'''\n",
    "class Model():\n",
    "    '''The init function is the initializing function of the class. This function automatically runs and need not be called separately\n",
    "    This function initializes all the required variables:\n",
    "      - DataGen - is the DataGenerator that must be passed to the class \n",
    "      - pid - list of Patient IDs depending on the mode\n",
    "      - n_splits: Cross_validation splits, default = 10\n",
    "      - path: path to save the Score Excel sheets\n",
    "      - mode: Mode of data to be obtained from the DataGenerator\n",
    "      - cache: path to cache storage folder\n",
    "      - name: names the file accoridngly. If not given, the name of the file is the same as pid\n",
    "\n",
    "    This function also creates the following variables while initializing the class:\n",
    "      - data, target: Data and labels obtained from calling the DataGenerator in the requested mode and pid\n",
    "      - feat_opts: Feature Reduction options of 75% and 25%\n",
    "      - metrics: scoring metrics of accuracy and f1\n",
    "      - scoring: a dictionary containing the scoring metrics\n",
    "      - order: Order of the score sheets tp be saved in the excel file\n",
    "      - border, col : used in formatting the excel file\n",
    "      - run_dict: miscellaneous variable to keep track of the feature reduction and label - val/aro/dom\n",
    "      - Classifiers: Dictionary containing all the Final Classifiers of the model\n",
    "      - Feature Scalars: Dictionary containing all the Feature Reduction Methods of the model\n",
    "    \n",
    "    All these variables are used in the model to help automate the entire process\n",
    "    '''\n",
    "    def __init__(self, DataGen, pid, n_splits=10, path='', mode='s_dep', cache='', name='chuck'):\n",
    "        self.path = path \n",
    "        self.cache = cache\n",
    "        self.mode = mode\n",
    "        self.pid = pid\n",
    "        self.n_splits = n_splits\n",
    "        self.D = DataGen\n",
    "        self.data, self.target = self.D.gen_data(self.mode, self.pid)\n",
    "        if type(self.pid) == type(['02']):\n",
    "          self.pid = '_'.join(self.pid)\n",
    "        if name!='chuck':\n",
    "          self.pid = name\n",
    "        print(self.data.shape, self.target.shape)\n",
    "        self.feat_opts = [0.75, 0.25]\n",
    "        self.metrics = ['accuracy', 'f1']\n",
    "        self.scoring = dict(zip(self.metrics,self.metrics))\n",
    "        self.order = ['75% Val', '25% Val', '75% Aro', '25% Aro', '75% Dom', '25% Dom']\n",
    "        self.border = Border(left=Side(style='thin'), right=Side(style='thin'), \n",
    "                     top=Side(style='thin'), bottom=Side(style='thin'))\n",
    "        self.col = dict(zip(list(range(1, 18)), list('ABCDEFGHIJ')))\n",
    "        self.run_dict = dict((zip(self.order, [(f, l) for l in range(3) for f in self.feat_opts] )))\n",
    "        self.Classifiers = {\n",
    "                'SVM_linear': svm.SVC(kernel='linear', cache_size=7000), 'SVM_rbf': svm.SVC(kernel='rbf', cache_size=7000),\n",
    "                'SVM_poly': svm.SVC(kernel='poly', cache_size=7000), 'SVM_sigmoid': svm.SVC(kernel='sigmoid', cache_size=7000),\n",
    "                'DecTree': DecisionTreeClassifier(), 'LogReg': LogisticRegression(solver='liblinear'), \n",
    "                'GNB': GaussianNB(), 'KNN': KNeighborsClassifier()}\n",
    "        self.FeatureScalers = {\n",
    "                'LDA': LDA(), 'RFE': RFE(svm.SVC(kernel='linear', cache_size=7000)), 'PCA': PCA(), 'FA': FactorAnalysis()}\n",
    "\n",
    "    #________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function to create an initial pipeline. \n",
    "    Input:\n",
    "      - fe: Name of Feature Elimination method\n",
    "      - clf: Name of Classifier\n",
    "      - n_features: 0.75 or 0.25 feature reduction\n",
    "    Output:\n",
    "      - Returns a full pipeline of the model with the input specifications\n",
    "    '''\n",
    "    def create_pipe(self, fe, clf, n_features):\n",
    "        fe_model = copy.deepcopy(self.FeatureScalers[fe])\n",
    "        if fe=='RFE':\n",
    "            fe_model.n_features_to_select = n_features\n",
    "        elif fe=='LDA':\n",
    "            pass\n",
    "        else:\n",
    "            fe_model.n_components = int(n_features*90)\n",
    "\n",
    "        pipe = Pipeline(steps=[\n",
    "                          ('Impute', SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=0)),\n",
    "                          ('Scale', StandardScaler()),\n",
    "                          ('Feature_Elim', fe_model),\n",
    "                          ('Classifier', copy.deepcopy(self.Classifiers[clf]))], \n",
    "                        memory = self.cache)\n",
    "        return pipe\n",
    "    #________________________________________________________________________________________________________\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function to get the cross_validation scores of the model\n",
    "    Input:\n",
    "      - model: Model or estimator to be fitted for cross_validation\n",
    "      - label: 0,1,2 for Val/Aro/Dom respectively\n",
    "    Output:\n",
    "      - returns the mean accuracy and mean f1 score after cross validation.\n",
    "    '''\n",
    "    def fit_score(self, model, label):\n",
    "        Model = copy.deepcopy(model)\n",
    "        scores = cross_validate(Model, self.data, self.target[:, label], scoring=self.scoring,\n",
    "                                 cv=StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=1), n_jobs=-1)\n",
    "        return round(np.mean(scores['test_accuracy']),4), round(np.mean(scores['test_f1']),4)\n",
    "    #________________________________________________________________________________________________________\n",
    "\n",
    "    '''\n",
    "    Function to create an empty dataframe with the Column Names and Row Names as required\n",
    "    Returns an empty DataFrame as follows:\n",
    "        | SVM1 | SVM2 | SVM3 | SVM4 | DT | LogReg | GNB | KNN |\n",
    "    LDA |                                                     |     \n",
    "    RFE |                                                     |\n",
    "    PCA |                                                     |\n",
    "    FA  |                                                     |\n",
    "    '''\n",
    "    def create_df(self):\n",
    "        #Create an empty Excel file for all the sheets to be added.\n",
    "        return pd.DataFrame(index = self.FeatureScalers.keys(), columns = self.Classifiers.keys())\n",
    "    #________________________________________________________________________________________________________\n",
    "    '''\n",
    "    Function to calculate scores for each model combination of feature elimination and classifier\n",
    "\n",
    "    Input:\n",
    "      - n_features: Feature Elimination 0.75 or 0.25 \n",
    "      - label: 0,1,2 for Val/Aro/Dom respectively\n",
    "    Output:\n",
    "      - Returns 2 dataframes for Accuracy and F1 scores respectively.\n",
    "    \n",
    "    The returned DataFrames are of this format\n",
    "\n",
    "    A/F | SVM1 | SVM2 | SVM3 | SVM4 | DT | LogReg | GNB | KNN | Max |\n",
    "    _________________________________________________________________\n",
    "    LDA |                                                           |     \n",
    "    RFE |                                                           |\n",
    "    PCA |                                                           |\n",
    "    FA  |                                                           |\n",
    "    _________________________________________________________________\n",
    "    Max |                                                           |\n",
    "    _________________________________________________________________\n",
    "\n",
    "    '''\n",
    "    def calc_scores(self, n_features, label):\n",
    "        #Create a dataframe to save the values\n",
    "        A_df = self.create_df()\n",
    "        F_df = self.create_df()\n",
    "        \n",
    "        #Iterate over the FeatureScalers - Feature Elimination Methods\n",
    "        for fred in self.FeatureScalers.keys():\n",
    "            print(\" \", end=\" \")\n",
    "            print(fred)\n",
    "            #Iterate over the Classifiers - Final Classifier Methods\n",
    "            for clf in self.Classifiers.keys():\n",
    "                print(clf, end=\"\\t\")\n",
    "                #Create a pipeline of the required Feature Elimination Method and final Classifier\n",
    "                Model = self.create_pipe(fred, clf, n_features)\n",
    "                #Calculate the cross_val score of the model\n",
    "                [a, f] = self.fit_score(Model, label)\n",
    "                #Save the score to the specific column and row of their respective dataframes \n",
    "                A_df.loc[fred, clf] = a\n",
    "                F_df.loc[fred, clf] = f\n",
    "            print()\n",
    "        \n",
    "        #Calculate the max columns\n",
    "        A_df['Max'] = A_df.max(axis=1)\n",
    "        A_df.loc['Max'] = A_df.max()\n",
    "        \n",
    "        F_df['Max'] = F_df.max(axis=1)\n",
    "        F_df.loc['Max'] = F_df.max()\n",
    "\n",
    "        #Completed message\n",
    "        print('\\n----> DF created: '+ str(n_features) + ' Features, Target - '+str(label))\n",
    "        return [A_df, F_df]\n",
    "\n",
    "    #The following 4 functions are not important- they are used for formatting the data in the excel sheet and saving\n",
    "    #________________________________________________________________________________________________________\n",
    "    #Functions for formatting the excel sheet\n",
    "    def apply_formatting(self, wb, metric):\n",
    "        if wb.worksheets[0].title==\"Sheet\":\n",
    "            wb.remove(wb.worksheets[0])\n",
    "        for ws in wb.worksheets:\n",
    "            ws['A1'] = metric\n",
    "            A = pd.DataFrame(ws.values).iloc[1:, 1:].to_numpy()\n",
    "            bold_idx = np.where(A == np.nanmax(A))\n",
    "            bold_cells = [self.col[i+2] + str(j+2) for i,j in zip(bold_idx[1], bold_idx[0])]\n",
    "            self.bold_colour(ws, bold_cells)\n",
    "            self.apply_border(ws)\n",
    "    \n",
    "    #Functions for formatting the excel sheet - to apply borders\n",
    "    def apply_border(self, sheet):\n",
    "        cells = [i + '6' for i in 'BCDEFGHIJ']\n",
    "        cells = cells + ['J' + str(i) for i in range(2,7)] + [i + '1' for i in 'ABCDEFGHIJ'] + ['A' + str(i) for i in range(2,7)]\n",
    "        for cell in cells:\n",
    "            sheet[cell].border = self.border\n",
    "    \n",
    "    #Functions for formatting the excel sheet - to highlight the cells of max accuracy        \n",
    "    def bold_colour(self, sheet, cells):\n",
    "        for cell in cells + [i + '1' for i in 'ABCDEFGHIJ'] + ['A' + str(i) for i in range(2,7)]:\n",
    "            sheet[cell].font = Font(bold=True)\n",
    "        CC = [c for c in cells if c[0]!='J' and c[1]!='6'] \n",
    "        my_fill = PatternFill(patternType='solid', fgColor='b6d7a8')\n",
    "        for cell in CC + ['A1']:\n",
    "            sheet[cell].fill = my_fill\n",
    "    \n",
    "    # Function that converts a pandas dataframe to an excel worksheet\n",
    "    def df2sheet(self, df, sheet):\n",
    "        for r in dataframe_to_rows(df, index=True, header=True):\n",
    "            sheet.append(r)\n",
    "        if not sheet['A2'].value:\n",
    "            sheet.delete_rows(2)\n",
    "    #________________________________________________________________________________________________________\n",
    "\n",
    "    '''\n",
    "    Main Function to Run\n",
    "\n",
    "    Function that Generates 2 Excel Files(Accuracy and F1) each with 6 sheets- \n",
    "    - '75% Val'\n",
    "    - '25% Val'\n",
    "    - '75% Aro'\n",
    "    - '25% Aro'\n",
    "    - '75% Dom'\n",
    "    - '25% Dom'\n",
    "    and saves it to the designated path.\n",
    "\n",
    "    '''\n",
    "    def run(self):\n",
    "        print('>>> Running Model for S' + self.pid)\n",
    "        acc = Workbook() #Opens new Excel workbook to save all the data\n",
    "        f1 = Workbook() #Opens new Excel workbook to save all the data\n",
    "        \n",
    "        #This for loop iterates over the sheet names\n",
    "        for name, value in zip(self.run_dict.keys(),self.run_dict.values()) :\n",
    "            '''Here, name is the sheet-name - obtained from the variable \"self.order\"\n",
    "            Value is (n_features, label) corresponding to the sheet name\n",
    "            For example: \n",
    "              - Sheetname - \"75% Val\"\n",
    "              - Value - (0.75, 0)\n",
    "            '''\n",
    "            #Create sheets in the acc and f1 workbooks for the sheetname\n",
    "            wA = acc.create_sheet(name)\n",
    "            wF = f1.create_sheet(name)\n",
    "\n",
    "            df = self.calc_scores(value[0], value[1]) # Calculates the scores of all the models and returns them as dataframes\n",
    "            self.df2sheet(df[0], wA) #Convert the dataframe to the required sheet\n",
    "            self.df2sheet(df[1], wF) #Convert the dataframe to the required sheet\n",
    "\n",
    "        #Apply formatting to the entire workbook - bold, max values and highlight\n",
    "        self.apply_formatting(acc, 'Acc') \n",
    "        self.apply_formatting(f1, 'F1')\n",
    "\n",
    "        #Save the workbook at the required path\n",
    "        acc.save(self.path + 'accuracy/s' + self.pid + '.xlsx')\n",
    "        f1.save(self.path + 'f1/s' + self.pid + '.xlsx')\n",
    "        print('>>> Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nczEGjzLzbIA"
   },
   "source": [
    "## Subject Dependent Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uU8Y1lX0WwB"
   },
   "outputs": [],
   "source": [
    "#Replace datapath to the folder containing the final features and add \"/feats\" to the end of the path\n",
    "#Replace metapath with the path to \"participant_questionnaire.csv\"\n",
    "\n",
    "#Create the DataGenerator Instance\n",
    "D = DataGenerator(datapath=\"/content/drive/MyDrive/Upwork/Final_features/feats\", \n",
    "                  metapath=\"/content/drive/MyDrive/Upwork/participant_questionnaire.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juvJEnoc2Aru"
   },
   "outputs": [],
   "source": [
    "# Change the path here - give the path of the folder you want to save the files to.\n",
    "# !!!Note : the folder should have an empty accuracy and f1 folder already made inside.\n",
    "path = \"/content/drive/MyDrive/Upwork/Results/Subject Dependent/\"\n",
    "\n",
    "#Change cache path as required\n",
    "cache_path = \"/content/Cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1srV2Hj1kH0"
   },
   "outputs": [],
   "source": [
    "#Run the model for each pid \n",
    "model = Model(D, pid='07', mode = 's_dep',\n",
    "              path=path, cache=cache_path)\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dfg3qRNl1j9b"
   },
   "outputs": [],
   "source": [
    "#Or use a for loop to run the model for all subjects\n",
    "for pid in ['01', '02', '03']:\n",
    "    model = Model(D, pid=pid, mode = 's_dep',\n",
    "                path=path, cache=cache_path)\n",
    "    model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NurT9J341jvj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAMTHIxQ0UDU"
   },
   "source": [
    "## Random Group Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avvHHgKLggBy"
   },
   "outputs": [],
   "source": [
    "#Replace datapath to the folder containing the final features and add \"/feats\" to the end of the path\n",
    "#Replace metapath with the path to \"participant_questionnaire.csv\"\n",
    "\n",
    "#Create the DataGenerator Instance\n",
    "D = DataGenerator(datapath=\"/content/drive/MyDrive/Upwork/Final_features/feats\", metapath=\"/content/drive/MyDrive/Upwork/participant_questionnaire.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k44UyyDznIcq"
   },
   "outputs": [],
   "source": [
    "#Run the DataGenerator at the required mode - s08/s16 to get the randomly selected groupings\n",
    "f,l = D.gen_data(mode='s08')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JzhuWJDo2eL"
   },
   "source": [
    "The Split Groupings are: \n",
    "- ['30', '23', '10', '05']\n",
    "- ['31', '04', '29', '16']\n",
    "- ['08', '09', '12', '24']\n",
    "- ['22', '17', '14', '13']\n",
    "- ['32', '06', '27', '01']\n",
    "- ['02', '11', '15', '25']\n",
    "- ['03', '26', '21', '19']\n",
    "- ['07', '18', '20', '28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdVJ9Onz1OY_"
   },
   "outputs": [],
   "source": [
    "# Change the path here - give the path of the folder you want to save the files to.\n",
    "# !!!Note : the folder should have an empty accuracy and f1 folder already made inside.\n",
    "random_path_s08 = \"/content/drive/MyDrive/Upwork/Results/Random Group Split/Splits_8/\"\n",
    "\n",
    "#Change cache path as required\n",
    "cache_path = \"/content/Cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQWusrxiggB1",
    "outputId": "1479d8b6-5fc7-4c33-93b9-041bd8f13f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 90) (5120, 3)\n"
     ]
    }
   ],
   "source": [
    "#Run the model for each split\n",
    "\n",
    "#For each run of the model - Change the input of the pid values and name as required. \n",
    "model = Model(D, pid=['30', '23', '10', '05'], mode = 's_dep', name='Group0',\n",
    "              path=random_path_s08, cache=cache_path) #Mode = s_dep: The model will concatenate all the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vX_1uBC5Dk_i",
    "outputId": "95b169aa-63aa-4fa4-d9da-e5d8e1fd5b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running Model for SGroup0\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.75 Features, Target - 0\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.25 Features, Target - 0\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.75 Features, Target - 1\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.25 Features, Target - 1\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.75 Features, Target - 2\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.25 Features, Target - 2\n",
      ">>> Completed\n"
     ]
    }
   ],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rWv257cRi-3",
    "outputId": "3af128b3-ef5c-4e4b-a967-b45083535fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 90) (5120, 3)\n",
      ">>> Running Model for SGroup1\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  FA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "\n",
      "----> DF created: 0.75 Features, Target - 0\n",
      "  LDA\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  RFE\n",
      "SVM_linear\tSVM_rbf\tSVM_poly\tSVM_sigmoid\tDecTree\tLogReg\tGNB\tKNN\t\n",
      "  PCA\n",
      "SVM_linear\tSVM_rbf\t"
     ]
    }
   ],
   "source": [
    "#For each run of the model - Change the input of the pid values and name as required. \n",
    "model = Model(D, pid=['31', '04', '29', '16'], mode = 's_dep', name='Group1',\n",
    "              path=\"/content/drive/MyDrive/Upwork/Results/Random Group Split/Splits_8/\", cache=\"/content/Cache\")\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJlFqluY1Wvq"
   },
   "source": [
    "**Similarly run the model for all random splits to get all the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/Aditi/Documents/Machine Learning/Upwork Project/data_preprocessed_python/data_preprocessed_python/Final_features/feats01.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 32, 5, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Scoring.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
